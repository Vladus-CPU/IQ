{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vladus-CPU/IQ/blob/main/work2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oiJrPBwOiUP",
        "outputId": "500c305e-5ef4-4b1f-c1d3-b92d2585a4dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Cannot install gradio==4.25.0 and huggingface-hub==0.14.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install --quiet colorcet diffusers gradio==4.25.0 hf-transfer huggingface-hub==0.14.0 matplotlib \\\n",
        "    numpy==1.23.5 opencv-contrib-python-headless pandas==2.0.0 Pillow==9.5.0 rich git+https://github.com/huggingface/pytorch-image-models.git@main#egg=timm \\\n",
        "    tokenizers torch>=2.1.0 torchvision transformers\n",
        "# При бажанні можете клонувати код з репозиторію, якщо він опублікований на GitHub,\n",
        "# або просто зкопіювати його у наступну комірку. Наведений нижче приклад показує,\n",
        "# як скопіювати файли безпосередньо:\n",
        "# !git clone https://github.com/user/wd-tagger-heatmap-example.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "# !pip install gradio\n",
        "import gradio as gr\n",
        "\n",
        "# Перевірка версії Gradio\n",
        "print(gr.__version__)\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from functools import lru_cache\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from huggingface_hub import hf_hub_download\n",
        "from huggingface_hub.utils import HfHubHTTPError\n",
        "from PIL import Image\n",
        "from torch import Tensor, nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import gradio as gr\n",
        "import cv2\n",
        "import colorcet as cc\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "import timm\n",
        "from timm.data import create_transform, resolve_data_config\n",
        "from timm.models import VisionTransformer\n",
        "from torchvision import transforms as T\n",
        "\n",
        "# ---------------------\n",
        "# BEGIN: common.py code\n",
        "# ---------------------\n",
        "\n",
        "@dataclass\n",
        "class Heatmap:\n",
        "    label: str\n",
        "    score: float\n",
        "    image: Image.Image\n",
        "\n",
        "@dataclass\n",
        "class LabelData:\n",
        "    names: list[str]\n",
        "    rating: list[np.int64]\n",
        "    general: list[np.int64]\n",
        "    character: list[np.int64]\n",
        "\n",
        "@dataclass\n",
        "class ImageLabels:\n",
        "    caption: str\n",
        "    booru: str\n",
        "    rating: dict[str, float]\n",
        "    general: dict[str, float]\n",
        "    character: dict[str, float]\n",
        "\n",
        "@lru_cache(maxsize=5)\n",
        "def load_labels_hf(repo_id: str, revision: Optional[str] = None, token: Optional[str] = None) -> LabelData:\n",
        "    try:\n",
        "        csv_path = hf_hub_download(\n",
        "            repo_id=repo_id,\n",
        "            filename=\"selected_tags.csv\",\n",
        "            revision=revision,\n",
        "            token=token\n",
        "        )\n",
        "        csv_path = Path(csv_path).resolve()\n",
        "    except HfHubHTTPError as e:\n",
        "        raise FileNotFoundError(f\"selected_tags.csv failed to download from {repo_id}\") from e\n",
        "\n",
        "    df: pd.DataFrame = pd.read_csv(csv_path, usecols=[\"name\", \"category\"])\n",
        "    tag_data = LabelData(\n",
        "        names=df[\"name\"].tolist(),\n",
        "        rating=list(np.where(df[\"category\"] == 9)[0]),\n",
        "        general=list(np.where(df[\"category\"] == 0)[0]),\n",
        "        character=list(np.where(df[\"category\"] == 4)[0]),\n",
        "    )\n",
        "\n",
        "    return tag_data\n",
        "\n",
        "def mcut_threshold(probs: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Maximum Cut Thresholding (MCut)\n",
        "    \"\"\"\n",
        "    probs = probs[probs.argsort()[::-1]]\n",
        "    diffs = probs[:-1] - probs[1:]\n",
        "    idx = diffs.argmax()\n",
        "    thresh = (probs[idx] + probs[idx + 1]) / 2\n",
        "    return float(thresh)\n",
        "\n",
        "def pil_ensure_rgb(image: Image.Image) -> Image.Image:\n",
        "    if image.mode not in [\"RGB\", \"RGBA\"]:\n",
        "        image = image.convert(\"RGBA\") if \"transparency\" in image.info else image.convert(\"RGB\")\n",
        "    if image.mode == \"RGBA\":\n",
        "        canvas = Image.new(\"RGBA\", image.size, (255, 255, 255))\n",
        "        canvas.alpha_composite(image)\n",
        "        image = canvas.convert(\"RGB\")\n",
        "    return image\n",
        "\n",
        "def pil_pad_square(image: Image.Image, fill: tuple[int, int, int] = (255, 255, 255)) -> Image.Image:\n",
        "    w, h = image.size\n",
        "    px = max(image.size)\n",
        "    canvas = Image.new(\"RGB\", (px, px), fill)\n",
        "    canvas.paste(image, ((px - w) // 2, (px - h) // 2))\n",
        "    return canvas\n",
        "\n",
        "def preprocess_image(image: Image.Image, size_px: int | tuple[int, int], upscale: bool = True) -> Image.Image:\n",
        "    if isinstance(size_px, int):\n",
        "        size_px = (size_px, size_px)\n",
        "    image = pil_ensure_rgb(image)\n",
        "    image = pil_pad_square(image)\n",
        "    if image.size[0] < size_px[0] or image.size[1] < size_px[1]:\n",
        "        if not upscale:\n",
        "            raise ValueError(\"Image is smaller than target size, and upscaling is disabled\")\n",
        "        image = image.resize(size_px, Image.LANCZOS)\n",
        "    if image.size[0] > size_px[0] or image.size[1] > size_px[1]:\n",
        "        image.thumbnail(size_px, Image.BICUBIC)\n",
        "    return image\n",
        "\n",
        "def pil_make_grid(\n",
        "    images: list[Image.Image],\n",
        "    max_cols: int = 8,\n",
        "    padding: int = 4,\n",
        "    bg_color: tuple[int, int, int] = (40, 42, 54),\n",
        "    partial_rows: bool = True,\n",
        ") -> Image.Image:\n",
        "    n_cols = min(math.floor(math.sqrt(len(images))), max_cols)\n",
        "    n_rows = math.ceil(len(images) / n_cols)\n",
        "    if n_cols * n_rows > len(images) and not partial_rows:\n",
        "        n_rows -= 1\n",
        "    image_width, image_height = images[0].size\n",
        "    canvas_width = ((image_width + padding) * n_cols) + padding\n",
        "    canvas_height = ((image_height + padding) * n_rows) + padding\n",
        "    canvas = Image.new(\"RGB\", (canvas_width, canvas_height), bg_color)\n",
        "    for i, img in enumerate(images):\n",
        "        x = (i % n_cols) * (image_width + padding) + padding\n",
        "        y = (i // n_cols) * (image_height + padding) + padding\n",
        "        canvas.paste(img, (x, y))\n",
        "    return canvas\n",
        "\n",
        "# ---------------------\n",
        "# END: common.py code\n",
        "# ---------------------\n",
        "\n",
        "# -----------------------\n",
        "# BEGIN: model.py code\n",
        "# -----------------------\n",
        "\n",
        "class RGBtoBGR(nn.Module):\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if x.ndim == 4:\n",
        "            return x[:, [2, 1, 0], :, :]\n",
        "        return x[[2, 1, 0], :, :]\n",
        "\n",
        "model_cache: dict[str, VisionTransformer] = {}\n",
        "transform_cache: dict[str, T.Compose] = {}\n",
        "\n",
        "def model_device(model: nn.Module) -> torch.device:\n",
        "    return next(model.parameters()).device\n",
        "\n",
        "def load_model(repo_id: str) -> VisionTransformer:\n",
        "    global model_cache\n",
        "    if model_cache.get(repo_id, None) is None:\n",
        "        model_cache[repo_id] = timm.create_model(\"hf-hub:\" + repo_id, pretrained=True).eval().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "    return model_cache[repo_id]\n",
        "\n",
        "def load_model_and_transform(repo_id: str) -> tuple[VisionTransformer, T.Compose]:\n",
        "    global transform_cache, model_cache\n",
        "    if model_cache.get(repo_id, None) is None:\n",
        "        model_cache[repo_id] = timm.create_model(\"hf-hub:\" + repo_id, pretrained=True).eval()\n",
        "    model = model_cache[repo_id]\n",
        "    if transform_cache.get(repo_id, None) is None:\n",
        "        transforms = create_transform(**resolve_data_config(model.pretrained_cfg, model=model))\n",
        "        transform_cache[repo_id] = T.Compose(transforms.transforms + [RGBtoBGR()])\n",
        "    return model, transform_cache[repo_id]\n",
        "\n",
        "def get_tags(\n",
        "    probs: Tensor,\n",
        "    labels: LabelData,\n",
        "    gen_threshold: float,\n",
        "    char_threshold: float,\n",
        "):\n",
        "    probs = list(zip(labels.names, probs.numpy()))\n",
        "    rating_labels = dict([probs[i] for i in labels.rating])\n",
        "    gen_labels = [probs[i] for i in labels.general]\n",
        "    gen_labels = dict([x for x in gen_labels if x[1] > gen_threshold])\n",
        "    gen_labels = dict(sorted(gen_labels.items(), key=lambda item: item[1], reverse=True))\n",
        "    char_labels = [probs[i] for i in labels.character]\n",
        "    char_labels = dict([x for x in char_labels if x[1] > char_threshold])\n",
        "    char_labels = dict(sorted(char_labels.items(), key=lambda item: item[1], reverse=True))\n",
        "    combined_names = [x for x in gen_labels]\n",
        "    combined_names.extend([x for x in char_labels])\n",
        "    caption = \", \".join(combined_names).replace(\"(\", \"\\\\(\").replace(\")\", \"\\\\)\")\n",
        "    booru = caption.replace(\"_\", \" \")\n",
        "    return caption, booru, rating_labels, char_labels, gen_labels\n",
        "\n",
        "@torch.no_grad()\n",
        "def render_heatmap(\n",
        "    image: Tensor,\n",
        "    gradients: Tensor,\n",
        "    image_feats: Tensor,\n",
        "    image_probs: Tensor,\n",
        "    image_labels: list[str],\n",
        "    cmap: LinearSegmentedColormap = cc.m_linear_bmy_10_95_c71,\n",
        "    pos_embed_dim: int = 784,\n",
        "    image_size: tuple[int, int] = (448, 448),\n",
        "    font_args: dict = {\n",
        "        \"fontFace\": cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        \"fontScale\": 1,\n",
        "        \"color\": (255, 255, 255),\n",
        "        \"thickness\": 2,\n",
        "        \"lineType\": cv2.LINE_AA,\n",
        "    },\n",
        "    partial_rows: bool = True,\n",
        ") -> tuple[list[Heatmap], Image.Image]:\n",
        "    image_hmaps = gradients.mean(2, keepdim=True).mul(image_feats.unsqueeze(0)).squeeze()\n",
        "    hmap_dim = int(math.sqrt(image_hmaps.mean(-1).numel() / len(image_labels)))\n",
        "    image_hmaps = image_hmaps.mean(-1).reshape(len(image_labels), -1)\n",
        "    image_hmaps = image_hmaps[..., -hmap_dim**2:]\n",
        "    image_hmaps = image_hmaps.reshape(len(image_labels), hmap_dim, hmap_dim)\n",
        "    image_hmaps = image_hmaps.max(torch.zeros_like(image_hmaps))\n",
        "    image_hmaps /= image_hmaps.reshape(image_hmaps.shape[0], -1).max(-1)[0].unsqueeze(-1).unsqueeze(-1)\n",
        "    image_hmaps = torch.stack([(x - x.min()) / (x.max() - x.min()) for x in image_hmaps]).unsqueeze(1)\n",
        "    image_hmaps = F.interpolate(image_hmaps, size=image_size, mode=\"bilinear\").squeeze(1)\n",
        "    hmap_imgs = []\n",
        "    for tag, hmap, score in zip(image_labels, image_hmaps, image_probs.cpu()):\n",
        "        image_pixels = image.add(1).mul(127.5).squeeze().permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
        "        hmap_pixels = cmap(hmap.cpu().numpy(), bytes=True)[:, :, :3]\n",
        "        hmap_cv2 = cv2.cvtColor(hmap_pixels, cv2.COLOR_RGB2BGR)\n",
        "        hmap_image = cv2.addWeighted(image_pixels, 0.5, hmap_cv2, 0.5, 0)\n",
        "        if tag is not None:\n",
        "            cv2.putText(hmap_image, tag, (10, 30), **font_args)\n",
        "            cv2.putText(hmap_image, f\"{score:.3f}\", (10, 60), **font_args)\n",
        "        hmap_pil = Image.fromarray(cv2.cvtColor(hmap_image, cv2.COLOR_BGR2RGB))\n",
        "        hmap_imgs.append(Heatmap(tag, score.item(), hmap_pil))\n",
        "    hmap_imgs = sorted(hmap_imgs, key=lambda x: x.score, reverse=True)\n",
        "    hmap_grid = pil_make_grid([x.image for x in hmap_imgs], partial_rows=partial_rows)\n",
        "    return hmap_imgs, hmap_grid\n",
        "\n",
        "def process_heatmap(\n",
        "    model: VisionTransformer,\n",
        "    image: Tensor,\n",
        "    labels: LabelData,\n",
        "    threshold: float = 0.5,\n",
        "    partial_rows: bool = True,\n",
        "):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    with torch.set_grad_enabled(True):\n",
        "        features = model.forward_features(image.to(device))\n",
        "        probs = model.forward_head(features)\n",
        "        probs = torch.sigmoid(probs).squeeze(0)\n",
        "        probs_mask = probs > threshold\n",
        "        heatmap_probs = probs[probs_mask]\n",
        "        label_indices = torch.nonzero(probs_mask, as_tuple=False).squeeze(1)\n",
        "        image_labels = [labels.names[label_indices[i]] for i in range(len(label_indices))]\n",
        "        eye = torch.eye(heatmap_probs.shape[0], device=device)\n",
        "        grads = torch.autograd.grad(\n",
        "            outputs=heatmap_probs,\n",
        "            inputs=features,\n",
        "            grad_outputs=eye,\n",
        "            is_grads_batched=True,\n",
        "            retain_graph=True,\n",
        "        )\n",
        "        grads = grads[0].detach().requires_grad_(False)[:, 0, :, :].unsqueeze(1)\n",
        "    with torch.set_grad_enabled(False):\n",
        "        hmap_imgs, hmap_grid = render_heatmap(\n",
        "            image=image,\n",
        "            gradients=grads,\n",
        "            image_feats=features,\n",
        "            image_probs=heatmap_probs,\n",
        "            image_labels=image_labels,\n",
        "            partial_rows=partial_rows,\n",
        "        )\n",
        "        caption, booru, ratings, character, general = get_tags(\n",
        "            probs=probs.cpu(),\n",
        "            labels=labels,\n",
        "            gen_threshold=threshold,\n",
        "            char_threshold=threshold,\n",
        "        )\n",
        "        image_labels_res = ImageLabels(caption, booru, ratings, general, character)\n",
        "    return hmap_imgs, hmap_grid, image_labels_res\n",
        "\n",
        "# -----------------------\n",
        "# END: model.py code\n",
        "# -----------------------\n",
        "\n",
        "# -----------------------\n",
        "# BEGIN: app.py code\n",
        "# -----------------------\n",
        "from os import getenv\n",
        "\n",
        "TITLE = \"WD Tagger Heatmap For More Models\"\n",
        "DESCRIPTION = \"\"\"WD Tagger v3 Heatmap Generator.\"\"\"\n",
        "HF_TOKEN = getenv(\"HF_TOKEN\", None)\n",
        "\n",
        "AVAILABLE_MODEL_REPOS = [\n",
        "    'SmilingWolf/wd-convnext-tagger-v3',\n",
        "    'SmilingWolf/wd-swinv2-tagger-v3',\n",
        "    'SmilingWolf/wd-vit-tagger-v3',\n",
        "    'SmilingWolf/wd-vit-large-tagger-v3',\n",
        "    \"SmilingWolf/wd-eva02-large-tagger-v3\",\n",
        "]\n",
        "MODEL_REPO = \"SmilingWolf/wd-vit-tagger-v3\"\n",
        "WORK_DIR = Path(\".\").resolve()\n",
        "IMAGE_EXTENSIONS = [\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\", \".bmp\", \".tiff\", \".tif\"]\n",
        "\n",
        "# Example images (can add paths to example images)\n",
        "example_images = []\n",
        "\n",
        "def predict(image: Image.Image, model_repo: str, threshold: float = 0.5):\n",
        "    model, transform = load_model_and_transform(model_repo)\n",
        "    labels: LabelData = load_labels_hf(model_repo)\n",
        "    image = preprocess_image(image, (448, 448))\n",
        "    image = transform(image).unsqueeze(0)\n",
        "    heatmaps, heatmap_grid, image_labels = process_heatmap(model, image, labels, threshold)\n",
        "    heatmap_images = [(x.image, x.label) for x in heatmaps]\n",
        "    return (\n",
        "        heatmap_images,\n",
        "        heatmap_grid,\n",
        "        image_labels.caption,\n",
        "        image_labels.booru,\n",
        "        image_labels.rating,\n",
        "        image_labels.character,\n",
        "        image_labels.general,\n",
        "    )\n",
        "\n",
        "# Enhanced CSS with scrollable functionality for Tags and Caption\n",
        "css = \"\"\"\n",
        "/* Styling for Caption Textbox */\n",
        "#caption_box {\n",
        "    max-height: 300px;             /* Sets a maximum height */\n",
        "    overflow-y: auto;              /* Adds vertical scroll if content exceeds max-height */\n",
        "    padding: 10px;                 /* Adds padding for better readability */\n",
        "    background-color: #3C3C3C;     /* Dark background for contrast */\n",
        "    color: #FFFFFF;                /* White text for readability */\n",
        "    border-radius: 1px;            /* Rounded corners */\n",
        "}\n",
        "\n",
        "/* Styling for Tags Textbox */\n",
        "#tags_box {\n",
        "    max-height: 300px;             /* Sets a maximum height */\n",
        "    overflow-y: auto;              /* Adds vertical scroll if content exceeds max-height */\n",
        "    padding: 10px;                 /* Adds padding for better readability */\n",
        "    background-color: #3C3C3C;     /* Dark background for contrast */\n",
        "    color: #FFFFFF;                /* White text for readability */\n",
        "    border-radius: 1px;            /* Rounded corners */\n",
        "}\n",
        "\n",
        "/* Optional: Customize Scrollbar Appearance */\n",
        "#caption_box::-webkit-scrollbar,\n",
        "#tags_box::-webkit-scrollbar {\n",
        "    width: 20px;                     /* Width of the scrollbar */\n",
        "}\n",
        "\n",
        "#caption_box::-webkit-scrollbar-track,\n",
        "#tags_box::-webkit-scrollbar-track {\n",
        "    background: #2D2D2D;            /* Track color */\n",
        "    border-radius: 20px;\n",
        "}\n",
        "\n",
        "#caption_box::-webkit-scrollbar-thumb,\n",
        "#tags_box::-webkit-scrollbar-thumb {\n",
        "    background-color: #4CAF50;      /* Scrollbar thumb color */\n",
        "    border-radius: 4px;\n",
        "    border: 3px solid #2D2D2D;      /* Adds a border around the thumb */\n",
        "}\n",
        "\n",
        "/* Responsive Design Adjustments */\n",
        "@media (max-width: 768px) {\n",
        "    #caption_box, #tags_box {\n",
        "        max-height: 100px;             /* Adjust max-height for smaller screens */\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(theme=\"default\", analytics_enabled=False, title=TITLE, css=css) as demo:\n",
        "    with gr.Row(equal_height=False):\n",
        "        with gr.Column(min_width=720):\n",
        "            with gr.Group():\n",
        "                img_input = gr.Image(\n",
        "                    label=\"Input\",\n",
        "                    type=\"pil\",\n",
        "                    image_mode=\"RGB\",\n",
        "                    sources=[\"upload\", \"clipboard\"],\n",
        "                )\n",
        "            with gr.Group():\n",
        "                with gr.Row():\n",
        "                    threshold = gr.Slider(\n",
        "                        minimum=0.0,\n",
        "                        maximum=1.0,\n",
        "                        value=0.35,\n",
        "                        step=0.01,\n",
        "                        label=\"Tag Threshold\",\n",
        "                        scale=5,\n",
        "                        elem_id=\"threshold\",\n",
        "                    )\n",
        "                    model_to_use = gr.Dropdown(\n",
        "                        choices=AVAILABLE_MODEL_REPOS,\n",
        "                        value=MODEL_REPO,\n",
        "                        label=\"Model Repository\"\n",
        "                    )\n",
        "            with gr.Row():\n",
        "                clear = gr.ClearButton(\n",
        "                    components=[img_input, heatmap_gallery, heatmap_grid, caption, tags, rating, character, general],\n",
        "                    variant=\"secondary\",\n",
        "                    size=\"lg\"\n",
        "                )\n",
        "                submit = gr.Button(value=\"Submit\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Column(min_width=720):\n",
        "            with gr.Tab(label=\"Heatmaps\"):\n",
        "                heatmap_gallery = gr.Gallery(columns=3, show_label=False)\n",
        "            with gr.Tab(label=\"Grid\"):\n",
        "                heatmap_grid = gr.Image(show_label=False)\n",
        "            with gr.Tab(label=\"Tags\"):\n",
        "                with gr.Group():\n",
        "                    caption = gr.Textbox(\n",
        "                        label=\"Caption\",\n",
        "                        show_copy_button=True,\n",
        "                        lines=20,                # Sets the number of visible lines\n",
        "                        elem_id=\"caption_box\"  # Assigned for CSS targeting\n",
        "                    )\n",
        "                    tags = gr.Textbox(\n",
        "                        label=\"Tags\",\n",
        "                        show_copy_button=True,\n",
        "                        lines=20,              # Sets the number of visible lines\n",
        "                        elem_id=\"tags_box\"   # Assigned for CSS targeting\n",
        "                    )\n",
        "                with gr.Group():\n",
        "                    rating = gr.Label(label=\"Rating\")\n",
        "                with gr.Group():\n",
        "                    character = gr.Label(label=\"Character\")\n",
        "                with gr.Group():\n",
        "                    general = gr.Label(label=\"General\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # Якщо у вас є приклади зображень, вкажіть їх у масиві example_images\n",
        "        example_inputs = [[img, MODEL_REPO, 0.35] for img in example_images]\n",
        "        examples = gr.Examples(\n",
        "            examples=example_inputs,\n",
        "            inputs=[img_input, model_to_use, threshold],\n",
        "        )\n",
        "\n",
        "    # Define clear button functionality\n",
        "    clear.click(\n",
        "        lambda: None,\n",
        "        outputs=[img_input, heatmap_gallery, heatmap_grid, caption, tags, rating, character, general],\n",
        "    )\n",
        "\n",
        "    # Define submit button functionality\n",
        "    submit.click(\n",
        "        predict,\n",
        "        inputs=[img_input, model_to_use, threshold],\n",
        "        outputs=[heatmap_gallery, heatmap_grid, caption, tags, rating, character, general],\n",
        "        api_name=\"predict\",\n",
        "    )\n",
        "\n",
        "# Launch the app\n",
        "demo.queue(max_size=10)\n",
        "demo.launch(server_name=\"0.0.0.0\", server_port=1200, debug=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "6AymO3FBOjVQ",
        "outputId": "4d3770da-fe5b-4717-d88a-59f05fc27d49"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.9.1\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://10e43b84272cd74416.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://10e43b84272cd74416.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}