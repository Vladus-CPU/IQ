{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vladus-CPU/IQ/blob/main/work2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------------\n",
        "# app.py (Extended version with artist style detection from \"Laxhar/noob-wiki\")\n",
        "# ----------------------------------------------------------------------------\n",
        "!pip install --quiet colorcet diffusers gradio hf-transfer huggingface-hub==0.14.0 matplotlib \\\n",
        "    numpy==1.23.5 opencv-contrib-python-headless pandas Pillow rich git+https://github.com/huggingface/pytorch-image-models.git@main#egg=timm\n",
        "!pip install tokenizers torch torchvision transformers\n",
        "!pip install numpy pandas torch torchvision timm datasets Pillow huggingface_hub opencv-python colorcet gradio\n",
        "!pip install numpy pandas torch torchvision timm gradio matplotlib colorcet pillow opencv-python datasets huggingface-hub\n",
        "!pip install pandas\n",
        "!pip install datasets huggingface_hub\n",
        "!pip install scikit-learn seaborn\n",
        "!pip install --upgrade pip\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from functools import lru_cache\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "\n",
        "import gradio as gr\n",
        "import cv2\n",
        "import colorcet as cc\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "import timm\n",
        "from timm.data import create_transform, resolve_data_config\n",
        "from timm.models import VisionTransformer\n",
        "from torchvision import transforms as T\n",
        "\n",
        "from PIL import Image\n",
        "from huggingface_hub import hf_hub_download\n",
        "from huggingface_hub.utils import HfHubHTTPError\n",
        "\n",
        "# Additional dataset import for artist style detection\n",
        "import datasets\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# BEGIN: Extended data classes and label loading\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class LabelData:\n",
        "    \"\"\"\n",
        "    Original label structure\n",
        "    \"\"\"\n",
        "    names: list[str]\n",
        "    rating: list[np.int64]\n",
        "    general: list[np.int64]\n",
        "    character: list[np.int64]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class LabelDataExtended:\n",
        "    \"\"\"\n",
        "    Extended label data to include artist styles\n",
        "    \"\"\"\n",
        "    names: list[str]\n",
        "    rating: list[np.int64]\n",
        "    general: list[np.int64]\n",
        "    character: list[np.int64]\n",
        "    artist_style: list[np.int64]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Heatmap:\n",
        "    label: str\n",
        "    score: float\n",
        "    image: Image.Image\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ImageLabels:\n",
        "    caption: str\n",
        "    booru: str\n",
        "    rating: dict[str, float]\n",
        "    general: dict[str, float]\n",
        "    character: dict[str, float]\n",
        "    # For convenience, you can add the artist labels here if needed:\n",
        "    # artist_style: dict[str, float]\n",
        "\n",
        "\n",
        "@lru_cache(maxsize=5)\n",
        "def load_labels_hf(repo_id: str, revision: Optional[str] = None, token: Optional[str] = None) -> LabelData:\n",
        "    \"\"\"\n",
        "    Loads the original label data from the Hugging Face repository (e.g. WD Tagger's CSV file).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        csv_path = hf_hub_download(\n",
        "            repo_id=repo_id,\n",
        "            filename=\"selected_tags.csv\",\n",
        "            revision=revision,\n",
        "            token=token\n",
        "        )\n",
        "        csv_path = Path(csv_path).resolve()\n",
        "    except HfHubHTTPError as e:\n",
        "        raise FileNotFoundError(f\"selected_tags.csv failed to download from {repo_id}\") from e\n",
        "\n",
        "    df: pd.DataFrame = pd.read_csv(csv_path, usecols=[\"name\", \"category\"])\n",
        "    tag_data = LabelData(\n",
        "        names=df[\"name\"].tolist(),\n",
        "        rating=list(np.where(df[\"category\"] == 9)[0]),\n",
        "        general=list(np.where(df[\"category\"] == 0)[0]),\n",
        "        character=list(np.where(df[\"category\"] == 4)[0]),\n",
        "    )\n",
        "    return tag_data\n",
        "\n",
        "\n",
        "@lru_cache(maxsize=5)\n",
        "def load_labels_ext(repo_id: str, revision: Optional[str] = None, token: Optional[str] = None) -> LabelDataExtended:\n",
        "    \"\"\"\n",
        "    Loads the extended label data with artist styles.\n",
        "    It contains original WD labels plus an extra category for \"artist style\"\n",
        "    from the \"Laxhar/noob-wiki\" dataset (as an example).\n",
        "    \"\"\"\n",
        "    # Load the base tag data from the WD Tagger\n",
        "    base_data = load_labels_hf(repo_id=repo_id, revision=revision, token=token)\n",
        "\n",
        "    # Example: load the additional dataset for artist style\n",
        "    # This is an illustrative example. Adjust to your dataset's actual format.\n",
        "    artist_ds = datasets.load_dataset(\"Laxhar/noob-wiki\", split=\"train\")\n",
        "\n",
        "    # Suppose the dataset has \"name\" and \"category\" columns,\n",
        "    # and \"category == 5\" indicates artist style.\n",
        "    # Adjust this logic to your actual dataset structure.\n",
        "    artist_style_indices = []\n",
        "    # In real usage, you'd probably have a separate set of names+indices\n",
        "    # or a method to map them onto your existing label set.\n",
        "    # Here we just demonstrate a possible approach:\n",
        "    for i, row in enumerate(artist_ds):\n",
        "        if \"category\" in row and row[\"category\"] == 5:\n",
        "            artist_style_indices.append(i)\n",
        "\n",
        "    # Create an extended data structure\n",
        "    return LabelDataExtended(\n",
        "        names=base_data.names,              # existing WD names\n",
        "        rating=base_data.rating,            # rating indices\n",
        "        general=base_data.general,          # general indices\n",
        "        character=base_data.character,      # character indices\n",
        "        artist_style=np.array(artist_style_indices, dtype=np.int64)  # new field\n",
        "    )\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# BEGIN: Utility functions\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def pil_ensure_rgb(image: Image.Image) -> Image.Image:\n",
        "    \"\"\"\n",
        "    Ensures the Pillow image is in a consistent RGB or RGBA mode.\n",
        "    \"\"\"\n",
        "    if image.mode not in [\"RGB\", \"RGBA\"]:\n",
        "        image = image.convert(\"RGBA\") if \"transparency\" in image.info else image.convert(\"RGB\")\n",
        "    if image.mode == \"RGBA\":\n",
        "        canvas = Image.new(\"RGBA\", image.size, (255, 255, 255))\n",
        "        canvas.alpha_composite(image)\n",
        "        image = canvas.convert(\"RGB\")\n",
        "    return image\n",
        "\n",
        "\n",
        "def pil_pad_square(image: Image.Image, fill: tuple[int, int, int] = (255, 255, 255)) -> Image.Image:\n",
        "    \"\"\"\n",
        "    Pads an image to a square with a given fill color.\n",
        "    \"\"\"\n",
        "    w, h = image.size\n",
        "    px = max(image.size)\n",
        "    canvas = Image.new(\"RGB\", (px, px), fill)\n",
        "    canvas.paste(image, ((px - w) // 2, (px - h) // 2))\n",
        "    return canvas\n",
        "\n",
        "\n",
        "def preprocess_image(image: Image.Image, size_px: int | tuple[int, int], upscale: bool = True) -> Image.Image:\n",
        "    \"\"\"\n",
        "    Preprocesses an image by converting to RGB, padding to a square,\n",
        "    and resizing/thumbnails to `size_px`.\n",
        "    \"\"\"\n",
        "    if isinstance(size_px, int):\n",
        "        size_px = (size_px, size_px)\n",
        "    image = pil_ensure_rgb(image)\n",
        "    image = pil_pad_square(image)\n",
        "    if image.size[0] < size_px[0] or image.size[1] < size_px[1]:\n",
        "        if not upscale:\n",
        "            raise ValueError(\"Image is smaller than target size, and upscaling is disabled\")\n",
        "        image = image.resize(size_px, Image.LANCZOS)\n",
        "    if image.size[0] > size_px[0] or image.size[1] > size_px[1]:\n",
        "        image.thumbnail(size_px, Image.BICUBIC)\n",
        "    return image\n",
        "\n",
        "\n",
        "def pil_make_grid(\n",
        "    images: list[Image.Image],\n",
        "    max_cols: int = 8,\n",
        "    padding: int = 4,\n",
        "    bg_color: tuple[int, int, int] = (40, 42, 54),\n",
        "    partial_rows: bool = True,\n",
        ") -> Image.Image:\n",
        "    \"\"\"\n",
        "    Creates a grid of images (like a contact sheet).\n",
        "    \"\"\"\n",
        "    n_cols = min(math.floor(math.sqrt(len(images))), max_cols)\n",
        "    n_rows = math.ceil(len(images) / n_cols)\n",
        "    if n_cols * n_rows > len(images) and not partial_rows:\n",
        "        n_rows -= 1\n",
        "    image_width, image_height = images[0].size\n",
        "    canvas_width = ((image_width + padding) * n_cols) + padding\n",
        "    canvas_height = ((image_height + padding) * n_rows) + padding\n",
        "    canvas = Image.new(\"RGB\", (canvas_width, canvas_height), bg_color)\n",
        "    for i, img in enumerate(images):\n",
        "        x = (i % n_cols) * (image_width + padding) + padding\n",
        "        y = (i // n_cols) * (image_height + padding) + padding\n",
        "        canvas.paste(img, (x, y))\n",
        "    return canvas\n",
        "\n",
        "\n",
        "def mcut_threshold(probs: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Maximum Cut Thresholding (MCut)\n",
        "    \"\"\"\n",
        "    probs = probs[probs.argsort()[::-1]]\n",
        "    diffs = probs[:-1] - probs[1:]\n",
        "    idx = diffs.argmax()\n",
        "    thresh = (probs[idx] + probs[idx + 1]) / 2\n",
        "    return float(thresh)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# BEGIN: Model creation and caching\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "class RGBtoBGR(nn.Module):\n",
        "    \"\"\"\n",
        "    Transforms an image from RGB to BGR channel order.\n",
        "    \"\"\"\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if x.ndim == 4:\n",
        "            return x[:, [2, 1, 0], :, :]\n",
        "        return x[[2, 1, 0], :, :]\n",
        "\n",
        "\n",
        "model_cache: dict[str, VisionTransformer] = {}\n",
        "transform_cache: dict[str, T.Compose] = {}\n",
        "\n",
        "\n",
        "def model_device(model: nn.Module) -> torch.device:\n",
        "    return next(model.parameters()).device\n",
        "\n",
        "\n",
        "def load_model(repo_id: str) -> VisionTransformer:\n",
        "    \"\"\"\n",
        "    Loads or retrieves the model from the timm HF Hub reference.\n",
        "    \"\"\"\n",
        "    global model_cache\n",
        "    if model_cache.get(repo_id, None) is None:\n",
        "        model_cache[repo_id] = (\n",
        "            timm.create_model(\"hf-hub:\" + repo_id, pretrained=True)\n",
        "            .eval()\n",
        "            .to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "        )\n",
        "    return model_cache[repo_id]\n",
        "\n",
        "\n",
        "def load_model_and_transform(repo_id: str) -> tuple[VisionTransformer, T.Compose]:\n",
        "    \"\"\"\n",
        "    Loads the model and its transforms (with optional RGB->BGR).\n",
        "    \"\"\"\n",
        "    global transform_cache, model_cache\n",
        "    if model_cache.get(repo_id, None) is None:\n",
        "        model_cache[repo_id] = timm.create_model(\"hf-hub:\" + repo_id, pretrained=True).eval()\n",
        "    model = model_cache[repo_id]\n",
        "    if transform_cache.get(repo_id, None) is None:\n",
        "        transforms = create_transform(**resolve_data_config(model.pretrained_cfg, model=model))\n",
        "        transform_cache[repo_id] = T.Compose(transforms.transforms + [RGBtoBGR()])\n",
        "    return model, transform_cache[repo_id]\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# BEGIN: Tag extraction with extended functionality\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def get_tags_extended(\n",
        "    probs: Tensor,\n",
        "    labels: LabelDataExtended,\n",
        "    gen_threshold: float,\n",
        "    char_threshold: float,\n",
        "    artist_threshold: float = 0.5,\n",
        "):\n",
        "    \"\"\"\n",
        "    Extended function to retrieve rating, general, character, and artist styles.\n",
        "    \"\"\"\n",
        "    probs_list = list(zip(labels.names, probs.numpy()))\n",
        "\n",
        "    # Original logic\n",
        "    rating_labels = dict([probs_list[i] for i in labels.rating])\n",
        "    gen_list = [probs_list[i] for i in labels.general]\n",
        "    gen_labels = dict([x for x in gen_list if x[1] > gen_threshold])\n",
        "    gen_labels = dict(sorted(gen_labels.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    char_list = [probs_list[i] for i in labels.character]\n",
        "    char_labels = dict([x for x in char_list if x[1] > char_threshold])\n",
        "    char_labels = dict(sorted(char_labels.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    # New logic for artist style:\n",
        "    style_list = [probs_list[i] for i in labels.artist_style if i < len(probs_list)]\n",
        "    style_labels = dict([x for x in style_list if x[1] > artist_threshold])\n",
        "    style_labels = dict(sorted(style_labels.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    # Combine names for caption\n",
        "    combined_names = list(gen_labels.keys()) + list(char_labels.keys()) + list(style_labels.keys())\n",
        "\n",
        "    caption = \", \".join(combined_names).replace(\"(\", \"\\(\").replace(\")\", \"\\)\")\n",
        "    booru = caption.replace(\"_\", \" \")\n",
        "\n",
        "    return caption, booru, rating_labels, char_labels, gen_labels, style_labels\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def render_heatmap(\n",
        "    image: Tensor,\n",
        "    gradients: Tensor,\n",
        "    image_feats: Tensor,\n",
        "    image_probs: Tensor,\n",
        "    image_labels: list[str],\n",
        "    cmap: LinearSegmentedColormap = cc.m_linear_bmy_10_95_c71,\n",
        "    image_size: tuple[int, int] = (448, 448),\n",
        "    font_args: dict = {\n",
        "        \"fontFace\": cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        \"fontScale\": 1,\n",
        "        \"color\": (255, 255, 255),\n",
        "        \"thickness\": 2,\n",
        "        \"lineType\": cv2.LINE_AA,\n",
        "    },\n",
        "    partial_rows: bool = True,\n",
        ") -> tuple[list[Heatmap], Image.Image]:\n",
        "    \"\"\"\n",
        "    Renders heatmaps by applying gradients to the image and creating\n",
        "    color overlays for each label discovered above threshold.\n",
        "    \"\"\"\n",
        "    # Reduce along tokens dimension\n",
        "    image_hmaps = gradients.mean(2, keepdim=True).mul(image_feats.unsqueeze(0)).squeeze()\n",
        "    hmap_dim = int(math.sqrt(image_hmaps.mean(-1).numel() / len(image_labels)))\n",
        "    image_hmaps = image_hmaps.mean(-1).reshape(len(image_labels), -1)\n",
        "    image_hmaps = image_hmaps[..., -hmap_dim**2:]\n",
        "    image_hmaps = image_hmaps.reshape(len(image_labels), hmap_dim, hmap_dim)\n",
        "    image_hmaps = image_hmaps.max(torch.zeros_like(image_hmaps))\n",
        "    image_hmaps /= image_hmaps.reshape(image_hmaps.shape[0], -1).max(-1)[0].unsqueeze(-1).unsqueeze(-1)\n",
        "    image_hmaps = torch.stack([(x - x.min()) / (x.max() - x.min()) for x in image_hmaps]).unsqueeze(1)\n",
        "    image_hmaps = F.interpolate(image_hmaps, size=image_size, mode=\"bilinear\").squeeze(1)\n",
        "\n",
        "    hmap_imgs = []\n",
        "    for tag, hmap, score in zip(image_labels, image_hmaps, image_probs.cpu()):\n",
        "        # Convert to 0..255 pixel range\n",
        "        image_pixels = image.add(1).mul(127.5).squeeze().permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
        "        hmap_pixels = cmap(hmap.cpu().numpy(), bytes=True)[:, :, :3]\n",
        "        hmap_cv2 = cv2.cvtColor(hmap_pixels, cv2.COLOR_RGB2BGR)\n",
        "        hmap_image = cv2.addWeighted(image_pixels, 0.5, hmap_cv2, 0.5, 0)\n",
        "        if tag is not None:\n",
        "            cv2.putText(hmap_image, tag, (10, 30), **font_args)\n",
        "            cv2.putText(hmap_image, f\"{score:.3f}\", (10, 60), **font_args)\n",
        "        hmap_pil = Image.fromarray(cv2.cvtColor(hmap_image, cv2.COLOR_BGR2RGB))\n",
        "        hmap_imgs.append(Heatmap(tag, score.item(), hmap_pil))\n",
        "\n",
        "    hmap_imgs = sorted(hmap_imgs, key=lambda x: x.score, reverse=True)\n",
        "    hmap_grid = pil_make_grid([x.image for x in hmap_imgs], partial_rows=partial_rows)\n",
        "    return hmap_imgs, hmap_grid\n",
        "\n",
        "\n",
        "def process_heatmap_extended(\n",
        "    model: VisionTransformer,\n",
        "    image: Tensor,\n",
        "    labels: LabelDataExtended,\n",
        "    threshold: float = 0.5,\n",
        "    partial_rows: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Core processing with the extended label data which includes artist style detection.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    with torch.set_grad_enabled(True):\n",
        "        features = model.forward_features(image.to(device))\n",
        "        probs = model.forward_head(features)\n",
        "        probs = torch.sigmoid(probs).squeeze(0)\n",
        "\n",
        "        # For demonstration, let's pick all tags over threshold:\n",
        "        probs_mask = probs > threshold\n",
        "        heatmap_probs = probs[probs_mask]\n",
        "        label_indices = torch.nonzero(probs_mask, as_tuple=False).squeeze(1)\n",
        "        image_labels = [labels.names[idx] for idx in label_indices if idx < len(labels.names)]\n",
        "\n",
        "        # Calculate gradients\n",
        "        eye = torch.eye(heatmap_probs.shape[0], device=device)\n",
        "        grads = torch.autograd.grad(\n",
        "            outputs=heatmap_probs,\n",
        "            inputs=features,\n",
        "            grad_outputs=eye,\n",
        "            is_grads_batched=True,\n",
        "            retain_graph=True,\n",
        "        )[0]\n",
        "        grads = grads.detach().requires_grad_(False)[:, 0, :, :].unsqueeze(1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Render heatmaps\n",
        "        hmap_imgs, hmap_grid = render_heatmap(\n",
        "            image=image,\n",
        "            gradients=grads,\n",
        "            image_feats=features,\n",
        "            image_probs=heatmap_probs,\n",
        "            image_labels=image_labels,\n",
        "            partial_rows=partial_rows,\n",
        "        )\n",
        "\n",
        "        # Get extended tags (including artist styles)\n",
        "        caption, booru, ratings, character, general, style_labels = get_tags_extended(\n",
        "            probs=probs.cpu(),\n",
        "            labels=labels,\n",
        "            gen_threshold=threshold,\n",
        "            char_threshold=threshold,\n",
        "            artist_threshold=threshold  # reuse same threshold or set a new one\n",
        "        )\n",
        "\n",
        "        # Create a result object for simpler return\n",
        "        image_labels_res = ImageLabels(\n",
        "            caption=caption,\n",
        "            booru=booru,\n",
        "            rating=ratings,\n",
        "            general=general,\n",
        "            character=character\n",
        "        )\n",
        "\n",
        "    # Optionally return style_labels if you want to display them separately\n",
        "    return hmap_imgs, hmap_grid, image_labels_res, style_labels\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# BEGIN: Gradio UI with extended functionality\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "from os import getenv\n",
        "\n",
        "TITLE = \"WD Tagger Heatmap + Artist Style Enhanced\"\n",
        "DESCRIPTION = \"\"\"Example Gradio app: WD Tagger with heatmap and additional detection for artist style.\"\"\"\n",
        "HF_TOKEN = getenv(\"HF_TOKEN\", None)\n",
        "\n",
        "AVAILABLE_MODEL_REPOS = [\n",
        "    'SmilingWolf/wd-convnext-tagger-v3',\n",
        "    'SmilingWolf/wd-swinv2-tagger-v3',\n",
        "    'SmilingWolf/wd-vit-tagger-v3',\n",
        "    'SmilingWolf/wd-vit-large-tagger-v3',\n",
        "    \"SmilingWolf/wd-eva02-large-tagger-v3\",\n",
        "]\n",
        "MODEL_REPO = \"SmilingWolf/wd-vit-tagger-v3\"\n",
        "\n",
        "WORK_DIR = Path(\".\").resolve()\n",
        "IMAGE_EXTENSIONS = [\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\", \".bmp\", \".tiff\", \".tif\"]\n",
        "\n",
        "# If you have example images, you can list them here\n",
        "example_images = []  # e.g., [\"examples/image1.png\", \"examples/image2.jpg\"]\n",
        "\n",
        "def predict_extended(image: Image.Image, model_repo: str, threshold: float = 0.5):\n",
        "    \"\"\"\n",
        "    Main inference function for Gradio UI that uses extended label data\n",
        "    for artist style detection.\n",
        "    \"\"\"\n",
        "    # Load the model and transforms\n",
        "    model, transform = load_model_and_transform(model_repo)\n",
        "    # Load extended label data\n",
        "    labels_ext = load_labels_ext(model_repo)\n",
        "    # Preprocess the input image\n",
        "    image = preprocess_image(image, (448, 448))\n",
        "    image_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "    # Process with extended logic\n",
        "    heatmaps, heatmap_grid, image_labels, style_labels = process_heatmap_extended(\n",
        "        model=model,\n",
        "        image=image_tensor,\n",
        "        labels=labels_ext,\n",
        "        threshold=threshold\n",
        "    )\n",
        "\n",
        "    heatmap_images = [(x.image, x.label) for x in heatmaps]\n",
        "\n",
        "    # Combine style labels into a separate dictionary or text if you want to present them\n",
        "    style_str = \", \".join([f\"{s} ({style_labels[s]:.3f})\" for s in style_labels])\n",
        "\n",
        "    return (\n",
        "        heatmap_images,\n",
        "        heatmap_grid,\n",
        "        image_labels.caption,\n",
        "        image_labels.booru,\n",
        "        image_labels.rating,\n",
        "        image_labels.character,\n",
        "        image_labels.general,\n",
        "        style_str\n",
        "    )\n",
        "\n",
        "css = \"\"\"\n",
        "#use_mcut, #char_mcut {\n",
        "    padding-top: var(--scale-3);\n",
        "}\n",
        "#threshold.dimmed {\n",
        "    filter: brightness(75%);\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(theme=\"default\", analytics_enabled=False, title=TITLE, css=css) as demo:\n",
        "    with gr.Row(equal_height=False):\n",
        "        with gr.Column(min_width=720):\n",
        "            with gr.Group():\n",
        "                img_input = gr.Image(\n",
        "                    label=\"Input\",\n",
        "                    type=\"pil\",\n",
        "                    image_mode=\"RGB\",\n",
        "                    sources=[\"upload\", \"clipboard\"],\n",
        "                )\n",
        "            with gr.Group():\n",
        "                with gr.Row():\n",
        "                    threshold = gr.Slider(\n",
        "                        minimum=0.0,\n",
        "                        maximum=1.0,\n",
        "                        value=0.35,\n",
        "                        step=0.01,\n",
        "                        label=\"Tag Threshold\",\n",
        "                        scale=5,\n",
        "                        elem_id=\"threshold\",\n",
        "                    )\n",
        "                    model_to_use = gr.Dropdown(choices=AVAILABLE_MODEL_REPOS, value=MODEL_REPO)\n",
        "            with gr.Row():\n",
        "                clear = gr.ClearButton(components=[], variant=\"secondary\", size=\"lg\")\n",
        "                submit = gr.Button(value=\"Submit\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Column(min_width=720):\n",
        "            with gr.Tab(label=\"Heatmaps\"):\n",
        "                heatmap_gallery = gr.Gallery(columns=3, show_label=False)\n",
        "            with gr.Tab(label=\"Grid\"):\n",
        "                heatmap_grid = gr.Image(show_label=False)\n",
        "            with gr.Tab(label=\"Tags\"):\n",
        "                with gr.Group():\n",
        "                    caption = gr.Textbox(label=\"Caption\", show_copy_button=True)\n",
        "                    tags = gr.Textbox(label=\"Tags\", show_copy_button=True, lines=2)\n",
        "                with gr.Group():\n",
        "                    rating = gr.Label(label=\"Rating\")\n",
        "                    character = gr.Label(label=\"Character\")\n",
        "                    general = gr.Label(label=\"General\")\n",
        "                    artist_style_str = gr.Textbox(label=\"Artist Style Detection\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # If you have example images, set them up for demonstration\n",
        "        example_inputs = [[img, MODEL_REPO, 0.35] for img in example_images]\n",
        "        examples = gr.Examples(\n",
        "            examples=example_inputs,\n",
        "            inputs=[img_input, model_to_use, threshold],\n",
        "        )\n",
        "\n",
        "    clear.add([img_input, heatmap_gallery, heatmap_grid, caption, tags, rating, character, general, artist_style_str])\n",
        "\n",
        "    submit.click(\n",
        "        predict_extended,\n",
        "        inputs=[img_input, model_to_use, threshold],\n",
        "        outputs=[\n",
        "            heatmap_gallery,\n",
        "            heatmap_grid,\n",
        "            caption,\n",
        "            tags,\n",
        "            rating,\n",
        "            character,\n",
        "            general,\n",
        "            artist_style_str\n",
        "        ],\n",
        "        api_name=\"predict_extended\",\n",
        "    )\n",
        "\n",
        "demo.queue(max_size=10)\n",
        "demo.launch(server_name=\"0.0.0.0\", server_port=7871, debug=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "6AymO3FBOjVQ",
        "outputId": "a6c1884a-ead1-4551-c46d-acee36e63f8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://00b68b32c48fc84c3e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://00b68b32c48fc84c3e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}