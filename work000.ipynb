{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vladus-CPU/IQ/blob/main/work000.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oiJrPBwOiUP",
        "outputId": "c65a04e0-61bb-47a7-fafb-a68e2d5802c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Cannot install gradio==4.25.0 and huggingface-hub==0.14.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --quiet colorcet diffusers gradio==4.25.0 hf-transfer huggingface-hub==0.14.0 matplotlib \\\n",
        "    numpy==1.23.5 opencv-contrib-python-headless pandas==2.0.0 Pillow==9.5.0 rich git+https://github.com/huggingface/pytorch-image-models.git@main#egg=timm \\\n",
        "    tokenizers torch>=2.1.0 torchvision transformers\n",
        "\n",
        "# При бажанні можете клонувати код з репозиторію, якщо він опублікований на GitHub,\n",
        "# або просто зкопіювати його у наступну комірку. Наведений нижче приклад показує,\n",
        "# як скопіювати файли безпосередньо:\n",
        "# !git clone https://github.com/user/wd-tagger-heatmap-example.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# To integrate logging, threshold flexibility, and caching,\n",
        "# this example adds:\n",
        "# 1) Basic logging setup\n",
        "# 2) A global prediction_cache for repeated calls\n",
        "# 3) A check in the predict(...) function to return cached\n",
        "#    results if the image and parameters are the same\n",
        "# 4) Logging messages at key steps\n",
        "# ---------------------------------------------------------\n",
        "!pip install gradio\n",
        "import gradio as gr\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from functools import lru_cache\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from huggingface_hub import hf_hub_download\n",
        "from huggingface_hub.utils import HfHubHTTPError\n",
        "from PIL import Image\n",
        "from torch import Tensor, nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import cv2\n",
        "import colorcet as cc\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "import timm\n",
        "from timm.data import create_transform, resolve_data_config\n",
        "from timm.models import VisionTransformer\n",
        "from torchvision import transforms as T\n",
        "from os import getenv\n",
        "\n",
        "# ---------------------\n",
        "# BEGIN: common.py code\n",
        "# ---------------------\n",
        "\n",
        "@dataclass\n",
        "class Heatmap:\n",
        "    label: str\n",
        "    score: float\n",
        "    image: Image.Image\n",
        "\n",
        "@dataclass\n",
        "class LabelData:\n",
        "    names: list[str]\n",
        "    rating: list[np.int64]\n",
        "    general: list[np.int64]\n",
        "    character: list[np.int64]\n",
        "\n",
        "@dataclass\n",
        "class ImageLabels:\n",
        "    caption: str\n",
        "    booru: str\n",
        "    rating: dict[str, float]\n",
        "    general: dict[str, float]\n",
        "    character: dict[str, float]\n",
        "\n",
        "@lru_cache(maxsize=5)\n",
        "def load_labels_hf(repo_id: str, revision: Optional[str] = None, token: Optional[str] = None) -> LabelData:\n",
        "    try:\n",
        "        csv_path = hf_hub_download(\n",
        "            repo_id=repo_id,\n",
        "            filename=\"selected_tags.csv\",\n",
        "            revision=revision,\n",
        "            token=token\n",
        "        )\n",
        "        csv_path = Path(csv_path).resolve()\n",
        "    except HfHubHTTPError as e:\n",
        "        raise FileNotFoundError(f\"selected_tags.csv failed to download from {repo_id}\") from e\n",
        "\n",
        "    df: pd.DataFrame = pd.read_csv(csv_path, usecols=[\"name\", \"category\"])\n",
        "    tag_data = LabelData(\n",
        "        names=df[\"name\"].tolist(),\n",
        "        rating=list(np.where(df[\"category\"] == 9)[0]),\n",
        "        general=list(np.where(df[\"category\"] == 0)[0]),\n",
        "        character=list(np.where(df[\"category\"] == 4)[0]),\n",
        "    )\n",
        "    return tag_data\n",
        "\n",
        "def mcut_threshold(probs: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Maximum Cut Thresholding (MCut)\n",
        "    \"\"\"\n",
        "    probs = probs[probs.argsort()[::-1]]\n",
        "    diffs = probs[:-1] - probs[1:]\n",
        "    idx = diffs.argmax()\n",
        "    thresh = (probs[idx] + probs[idx + 1]) / 2\n",
        "    return float(thresh)\n",
        "\n",
        "def pil_ensure_rgb(image: Image.Image) -> Image.Image:\n",
        "    if image.mode not in [\"RGB\", \"RGBA\"]:\n",
        "        image = image.convert(\"RGBA\") if \"transparency\" in image.info else image.convert(\"RGB\")\n",
        "    if image.mode == \"RGBA\":\n",
        "        canvas = Image.new(\"RGBA\", image.size, (255, 255, 255))\n",
        "        canvas.alpha_composite(image)\n",
        "        image = canvas.convert(\"RGB\")\n",
        "    return image\n",
        "\n",
        "def pil_pad_square(image: Image.Image, fill: tuple[int, int, int] = (255, 255, 255)) -> Image.Image:\n",
        "    w, h = image.size\n",
        "    px = max(image.size)\n",
        "    canvas = Image.new(\"RGB\", (px, px), fill)\n",
        "    canvas.paste(image, ((px - w) // 2, (px - h) // 2))\n",
        "    return canvas\n",
        "\n",
        "def preprocess_image(image: Image.Image, size_px: int | tuple[int,int], upscale: bool = True) -> Image.Image:\n",
        "    if isinstance(size_px, int):\n",
        "        size_px = (size_px, size_px)\n",
        "    image = pil_ensure_rgb(image)\n",
        "    image = pil_pad_square(image)\n",
        "    if image.size[0] < size_px[0] or image.size[1] < size_px[1]:\n",
        "        if not upscale:\n",
        "            raise ValueError(\"Image is smaller than target size, and upscaling is disabled\")\n",
        "        image = image.resize(size_px, Image.LANCZOS)\n",
        "    if image.size[0] > size_px[0] or image.size[1] > size_px[1]:\n",
        "        image.thumbnail(size_px, Image.BICUBIC)\n",
        "    return image\n",
        "\n",
        "def pil_make_grid(\n",
        "    images: list[Image.Image],\n",
        "    max_cols: int = 8,\n",
        "    padding: int = 4,\n",
        "    bg_color: tuple[int, int, int] = (40, 42, 54),\n",
        "    partial_rows: bool = True,\n",
        ") -> Image.Image:\n",
        "    n_cols = min(math.floor(math.sqrt(len(images))), max_cols)\n",
        "    n_rows = math.ceil(len(images) / n_cols)\n",
        "    if n_cols * n_rows > len(images) and not partial_rows:\n",
        "        n_rows -= 1\n",
        "    image_width, image_height = images[0].size\n",
        "    canvas_width = ((image_width + padding) * n_cols) + padding\n",
        "    canvas_height = ((image_height + padding) * n_rows) + padding\n",
        "    canvas = Image.new(\"RGB\", (canvas_width, canvas_height), bg_color)\n",
        "    for i, img in enumerate(images):\n",
        "        x = (i % n_cols) * (image_width + padding) + padding\n",
        "        y = (i // n_cols) * (image_height + padding) + padding\n",
        "        canvas.paste(img, (x, y))\n",
        "    return canvas\n",
        "\n",
        "# ---------------------\n",
        "# END: common.py code\n",
        "# ---------------------\n",
        "\n",
        "# -----------------------\n",
        "# BEGIN: model.py code\n",
        "# -----------------------\n",
        "\n",
        "class RGBtoBGR(nn.Module):\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if x.ndim == 4:\n",
        "            return x[:, [2, 1, 0], :, :]\n",
        "        return x[[2, 1, 0], :, :]\n",
        "\n",
        "model_cache: dict[str, VisionTransformer] = {}\n",
        "transform_cache: dict[str, T.Compose] = {}\n",
        "\n",
        "def model_device(model: nn.Module) -> torch.device:\n",
        "    return next(model.parameters()).device\n",
        "\n",
        "def load_model(repo_id: str) -> VisionTransformer:\n",
        "    global model_cache\n",
        "    if model_cache.get(repo_id, None) is None:\n",
        "        logging.info(f\"Loading model from {repo_id} for the first time...\")\n",
        "        model_cache[repo_id] = timm.create_model(\"hf-hub:\" + repo_id, pretrained=True).eval().to(\n",
        "            torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        )\n",
        "    else:\n",
        "        logging.info(f\"Model for {repo_id} found in cache, using cached version.\")\n",
        "    return model_cache[repo_id]\n",
        "\n",
        "def load_model_and_transform(repo_id: str) -> tuple[VisionTransformer, T.Compose]:\n",
        "    global transform_cache, model_cache\n",
        "    model = load_model(repo_id)\n",
        "    if transform_cache.get(repo_id, None) is None:\n",
        "        logging.info(\"Creating transform pipeline...\")\n",
        "        transforms = create_transform(**resolve_data_config(model.pretrained_cfg, model=model))\n",
        "        transform_cache[repo_id] = T.Compose(transforms.transforms + [RGBtoBGR()])\n",
        "    else:\n",
        "        logging.info(\"Transform pipeline is cached, using existing one.\")\n",
        "    return model, transform_cache[repo_id]\n",
        "\n",
        "def get_tags(\n",
        "    probs: Tensor,\n",
        "    labels: LabelData,\n",
        "    gen_threshold: float,\n",
        "    char_threshold: float,\n",
        "):\n",
        "    probs = list(zip(labels.names, probs.numpy()))\n",
        "    rating_labels = dict([probs[i] for i in labels.rating])\n",
        "    gen_labels = [probs[i] for i in labels.general]\n",
        "    gen_labels = dict([x for x in gen_labels if x[1] > gen_threshold])\n",
        "    gen_labels = dict(sorted(gen_labels.items(), key=lambda item: item[1], reverse=True))\n",
        "    char_labels = [probs[i] for i in labels.character]\n",
        "    char_labels = dict([x for x in char_labels if x[1] > char_threshold])\n",
        "    char_labels = dict(sorted(char_labels.items(), key=lambda item: item[1], reverse=True))\n",
        "    combined_names = [x for x in gen_labels]\n",
        "    combined_names.extend([x for x in char_labels])\n",
        "    caption = \", \".join(combined_names).replace(\"(\", \"\\(\").replace(\")\", \"\\)\")\n",
        "    booru = caption.replace(\"_\", \" \")\n",
        "    return caption, booru, rating_labels, char_labels, gen_labels\n",
        "\n",
        "@torch.no_grad()\n",
        "def render_heatmap(\n",
        "    image: Tensor,\n",
        "    gradients: Tensor,\n",
        "    image_feats: Tensor,\n",
        "    image_probs: Tensor,\n",
        "    image_labels: list[str],\n",
        "    cmap: LinearSegmentedColormap = cc.m_linear_bmy_10_95_c71,\n",
        "    pos_embed_dim: int = 784,\n",
        "    image_size: tuple[int, int] = (448, 448),\n",
        "    font_args: dict = {\n",
        "        \"fontFace\": cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        \"fontScale\": 1,\n",
        "        \"color\": (255, 255, 255),\n",
        "        \"thickness\": 2,\n",
        "        \"lineType\": cv2.LINE_AA,\n",
        "    },\n",
        "    partial_rows: bool = True,\n",
        ") -> tuple[list[Heatmap], Image.Image]:\n",
        "    image_hmaps = gradients.mean(2, keepdim=True).mul(image_feats.unsqueeze(0)).squeeze()\n",
        "    hmap_dim = int(math.sqrt(image_hmaps.mean(-1).numel() / len(image_labels)))\n",
        "    image_hmaps = image_hmaps.mean(-1).reshape(len(image_labels), -1)\n",
        "    image_hmaps = image_hmaps[..., -hmap_dim**2:]\n",
        "    image_hmaps = image_hmaps.reshape(len(image_labels), hmap_dim, hmap_dim)\n",
        "    image_hmaps = image_hmaps.max(torch.zeros_like(image_hmaps))\n",
        "    image_hmaps /= image_hmaps.reshape(image_hmaps.shape[0], -1).max(-1)[0].unsqueeze(-1).unsqueeze(-1)\n",
        "    image_hmaps = torch.stack([(x - x.min()) / (x.max() - x.min()) for x in image_hmaps]).unsqueeze(1)\n",
        "    image_hmaps = F.interpolate(image_hmaps, size=image_size, mode=\"bilinear\").squeeze(1)\n",
        "    hmap_imgs = []\n",
        "    for tag, hmap, score in zip(image_labels, image_hmaps, image_probs.cpu()):\n",
        "        image_pixels = image.add(1).mul(127.5).squeeze().permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
        "        hmap_pixels = cmap(hmap.cpu().numpy(), bytes=True)[:, :, :3]\n",
        "        hmap_cv2 = cv2.cvtColor(hmap_pixels, cv2.COLOR_RGB2BGR)\n",
        "        hmap_image = cv2.addWeighted(image_pixels, 0.5, hmap_cv2, 0.5, 0)\n",
        "        if tag is not None:\n",
        "            cv2.putText(hmap_image, tag, (10, 30), **font_args)\n",
        "            cv2.putText(hmap_image, f\"{score:.3f}\", (10, 60), **font_args)\n",
        "        hmap_pil = Image.fromarray(cv2.cvtColor(hmap_image, cv2.COLOR_BGR2RGB))\n",
        "        hmap_imgs.append(Heatmap(tag, score.item(), hmap_pil))\n",
        "    hmap_imgs = sorted(hmap_imgs, key=lambda x: x.score, reverse=True)\n",
        "    hmap_grid = pil_make_grid([x.image for x in hmap_imgs], partial_rows=partial_rows)\n",
        "    return hmap_imgs, hmap_grid\n",
        "\n",
        "def process_heatmap(\n",
        "    model: VisionTransformer,\n",
        "    image: Tensor,\n",
        "    labels: LabelData,\n",
        "    threshold: float = 0.5,\n",
        "    partial_rows: bool = True,\n",
        "):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    with torch.set_grad_enabled(True):\n",
        "        features = model.forward_features(image.to(device))\n",
        "        probs = model.forward_head(features)\n",
        "        probs = torch.sigmoid(probs).squeeze(0)\n",
        "        probs_mask = probs > threshold\n",
        "        heatmap_probs = probs[probs_mask]\n",
        "        label_indices = torch.nonzero(probs_mask, as_tuple=False).squeeze(1)\n",
        "        image_labels = [labels.names[label_indices[i]] for i in range(len(label_indices))]\n",
        "        eye = torch.eye(heatmap_probs.shape[0], device=device)\n",
        "        grads = torch.autograd.grad(\n",
        "            outputs=heatmap_probs,\n",
        "            inputs=features,\n",
        "            grad_outputs=eye,\n",
        "            is_grads_batched=True,\n",
        "            retain_graph=True,\n",
        "        )\n",
        "        grads = grads[0].detach().requires_grad_(False)[:, 0, :, :].unsqueeze(1)\n",
        "    with torch.set_grad_enabled(False):\n",
        "        hmap_imgs, hmap_grid = render_heatmap(\n",
        "            image=image,\n",
        "            gradients=grads,\n",
        "            image_feats=features,\n",
        "            image_probs=heatmap_probs,\n",
        "            image_labels=image_labels,\n",
        "            partial_rows=partial_rows,\n",
        "        )\n",
        "        caption, booru, ratings, character, general = get_tags(\n",
        "            probs=probs.cpu(),\n",
        "            labels=labels,\n",
        "            gen_threshold=threshold,\n",
        "            char_threshold=threshold,\n",
        "        )\n",
        "        image_labels_res = ImageLabels(caption, booru, ratings, general, character)\n",
        "    return hmap_imgs, hmap_grid, image_labels_res\n",
        "\n",
        "# -----------------------\n",
        "# END: model.py code\n",
        "# -----------------------\n",
        "\n",
        "# -----------------------\n",
        "# BEGIN: app.py code\n",
        "# -----------------------\n",
        "\n",
        "TITLE = \"WD Tagger Heatmap For More Models\"\n",
        "DESCRIPTION = \"\"\"WD Tagger v3 Heatmap Generator.\"\"\"\n",
        "HF_TOKEN = getenv(\"HF_TOKEN\", None)\n",
        "\n",
        "AVAILABLE_MODEL_REPOS = [\n",
        "    'SmilingWolf/wd-convnext-tagger-v3',\n",
        "    'SmilingWolf/wd-swinv2-tagger-v3',\n",
        "    'SmilingWolf/wd-vit-tagger-v3',\n",
        "    'SmilingWolf/wd-vit-large-tagger-v3',\n",
        "    \"SmilingWolf/wd-eva02-large-tagger-v3\",\n",
        "]\n",
        "MODEL_REPO = \"SmilingWolf/wd-vit-tagger-v3\"\n",
        "WORK_DIR = Path(\".\").resolve()\n",
        "IMAGE_EXTENSIONS = [\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\", \".bmp\", \".tiff\", \".tif\"]\n",
        "\n",
        "# Example: You can put images in the \"examples\" folder if desired\n",
        "example_images = []\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Global dictionary to cache predictions across calls\n",
        "# Key could be (hashed_image, model_repo, threshold)\n",
        "# ---------------------------------------------------------\n",
        "prediction_cache = {}\n",
        "\n",
        "def predict(image: Image.Image, model_repo: str, threshold: float = 0.5):\n",
        "    \"\"\"\n",
        "    Main function for Gradio:\n",
        "    1) Uses a global cache to store repeated predictions.\n",
        "    2) Loads model and transforms if not cached already.\n",
        "    3) Preprocesses the image.\n",
        "    4) Runs process_heatmap(...) to generate heatmaps and tags.\n",
        "    \"\"\"\n",
        "    if image is None:\n",
        "        return None, None, \"\", \"\", {}, {}, {}\n",
        "\n",
        "    # Generate a unique key for caching based on the image contents\n",
        "    # plus the model_repo and threshold.\n",
        "    image_key = hash(image.tobytes())\n",
        "    cache_key = (image_key, model_repo, threshold)\n",
        "\n",
        "    # If we have a cached result, return it immediately\n",
        "    if cache_key in prediction_cache:\n",
        "        logging.info(\"Returning result from prediction cache.\")\n",
        "        return prediction_cache[cache_key]\n",
        "\n",
        "    logging.info(f\"Processing new prediction: Model={model_repo}, Threshold={threshold}\")\n",
        "\n",
        "    model, transform = load_model_and_transform(model_repo)\n",
        "    labels: LabelData = load_labels_hf(model_repo)\n",
        "    img_proc = preprocess_image(image, (448, 448))\n",
        "    img_tensor = transform(img_proc).unsqueeze(0)\n",
        "\n",
        "    heatmaps, heatmap_grid, image_labels = process_heatmap(model, img_tensor, labels, threshold)\n",
        "\n",
        "    # Format the results so Gradio can handle them\n",
        "    heatmap_images = [(x.image, x.label) for x in heatmaps]\n",
        "    output = (\n",
        "        heatmap_images,\n",
        "        heatmap_grid,\n",
        "        image_labels.caption,\n",
        "        image_labels.booru,\n",
        "        image_labels.rating,\n",
        "        image_labels.character,\n",
        "        image_labels.general,\n",
        "    )\n",
        "\n",
        "    # Save to cache before returning\n",
        "    prediction_cache[cache_key] = output\n",
        "    return output\n",
        "\n",
        "css = \"\"\"\n",
        "#use_mcut, #char_mcut {\n",
        "    padding-top: var(--scale-3);\n",
        "}\n",
        "#threshold.dimmed {\n",
        "    filter: brightness(75%);\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(theme=\"default\", analytics_enabled=False, title=TITLE, css=css) as demo:\n",
        "    with gr.Row(equal_height=False):\n",
        "        with gr.Column(min_width=720):\n",
        "            with gr.Group():\n",
        "                img_input = gr.Image(\n",
        "                    label=\"Input\",\n",
        "                    type=\"pil\",\n",
        "                    image_mode=\"RGB\",\n",
        "                    sources=[\"upload\", \"clipboard\"]\n",
        "                )\n",
        "            with gr.Group():\n",
        "                with gr.Row():\n",
        "                    threshold = gr.Slider(\n",
        "                        minimum=0.0,\n",
        "                        maximum=1.0,\n",
        "                        value=0.35,\n",
        "                        step=0.01,\n",
        "                        label=\"Tag Threshold\",\n",
        "                        scale=5,\n",
        "                        elem_id=\"threshold\",\n",
        "                    )\n",
        "                    model_to_use = gr.Dropdown(\n",
        "                        choices=AVAILABLE_MODEL_REPOS,\n",
        "                        value=MODEL_REPO\n",
        "                    )\n",
        "            with gr.Row():\n",
        "                clear = gr.ClearButton(components=[], variant=\"secondary\", size=\"lg\")\n",
        "                submit = gr.Button(value=\"Submit\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Column(min_width=720):\n",
        "            with gr.Tab(label=\"Heatmaps\"):\n",
        "                heatmap_gallery = gr.Gallery(columns=3, show_label=False)\n",
        "            with gr.Tab(label=\"Grid\"):\n",
        "                heatmap_grid = gr.Image(show_label=False)\n",
        "            with gr.Tab(label=\"Tags\"):\n",
        "                with gr.Group():\n",
        "                    caption = gr.Textbox(label=\"Caption\", show_copy_button=True)\n",
        "                    tags = gr.Textbox(label=\"Tags\", show_copy_button=True)\n",
        "                with gr.Group():\n",
        "                    rating = gr.Label(label=\"Rating\")\n",
        "                with gr.Group():\n",
        "                    character = gr.Label(label=\"Character\")\n",
        "                with gr.Group():\n",
        "                    general = gr.Label(label=\"General\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # If you have example images, specify them in example_images array\n",
        "        example_inputs = [[img, MODEL_REPO, 0.35] for img in example_images]\n",
        "        examples = gr.Examples(\n",
        "            examples=example_inputs,\n",
        "            inputs=[img_input, model_to_use, threshold],\n",
        "        )\n",
        "\n",
        "    clear.add([img_input, heatmap_gallery, heatmap_grid, caption, tags, rating, character, general])\n",
        "    submit.click(\n",
        "        predict,\n",
        "        inputs=[img_input, model_to_use, threshold],\n",
        "        outputs=[heatmap_gallery, heatmap_grid, caption, tags, rating, character, general],\n",
        "        api_name=\"predict\",\n",
        "    )\n",
        "\n",
        "# For local testing or running on a server:\n",
        "demo.queue(max_size=10)\n",
        "demo.launch(server_name=\"0.0.0.0\", server_port=7871, debug=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6AymO3FBOjVQ",
        "outputId": "e969bc77-50bb-4ac6-fff0-d7c25f3be4be"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.12.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.5.4 (from gradio)\n",
            "  Downloading gradio_client-1.5.4-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.9.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.4->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.4->gradio) (14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.12.0-py3-none-any.whl (57.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.4-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.4/321.4 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m123.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.6 ffmpy-0.5.0 gradio-5.12.0 gradio-client-1.5.4 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.1 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.34.0\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c720ce6d7ec9020e25.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c720ce6d7ec9020e25.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}