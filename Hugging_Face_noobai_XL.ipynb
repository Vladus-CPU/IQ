{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlimited Token Stable Diffusion XL with noobai-XL-1.1\n",
    "\n",
    "**Author:** Vladus-CPU  \n",
    "**Last Updated:** 2025-01-01 22:03:57 UTC\n",
    "\n",
    "This notebook provides implementation of Stable Diffusion XL with unlimited token length for prompts using the noobai-XL-1.1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade diffusers transformers torch torchvision safetensors accelerate xformers matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline, EulerAncestralDiscreteScheduler, DDIMScheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pipeline_without_limits(scheduler_type=\"Euler\"):\n",
    "    \"\"\"Load the noobai-XL-1.1 pipeline with unlimited token length.\"\"\"\n",
    "    model_id = \"Laxhar/noobai-XL-1.1\"\n",
    "    \n",
    "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        safety_checker=None,\n",
    "        use_safetensors=True\n",
    "    )\n",
    "    \n",
    "    # Remove token length limits\n",
    "    if hasattr(pipe, 'tokenizer') and pipe.tokenizer is not None:\n",
    "        pipe.tokenizer.model_max_length = int(1e9)\n",
    "        if hasattr(pipe.tokenizer, 'max_length'):\n",
    "            pipe.tokenizer.max_length = int(1e9)\n",
    "            \n",
    "    if hasattr(pipe, 'tokenizer_2') and pipe.tokenizer_2 is not None:\n",
    "        pipe.tokenizer_2.model_max_length = int(1e9)\n",
    "        if hasattr(pipe.tokenizer_2, 'max_length'):\n",
    "            pipe.tokenizer_2.max_length = int(1e9)\n",
    "    \n",
    "    # Set scheduler\n",
    "    if scheduler_type == \"Euler\":\n",
    "        pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "    elif scheduler_type == \"DDIM\":\n",
    "        pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown scheduler type\")\n",
    "    \n",
    "    # Optimizations\n",
    "    if hasattr(pipe, \"enable_xformers_memory_efficient_attention\"):\n",
    "        pipe.enable_xformers_memory_efficient_attention()\n",
    "    \n",
    "    if hasattr(pipe, \"enable_vae_tiling\"):\n",
    "        pipe.enable_vae_tiling()\n",
    "    \n",
    "    # Device placement\n",
    "    if device.type == \"cuda\" and torch.cuda.get_device_properties(0).total_memory < 12 * 1024 * 1024 * 1024:\n",
    "        pipe.enable_model_cpu_offload()\n",
    "    else:\n",
    "        pipe = pipe.to(device)\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(\n",
    "    prompt,\n",
    "    negative_prompt=\"\",\n",
    "    sampler=\"Euler\",\n",
    "    sampling_steps=30,\n",
    "    seed=None,\n",
    "    cfg_scale=7.0,\n",
    "    image_size=(1024, 1024),\n",
    "    hires_fix=False,\n",
    "    hires_scale=1.5,\n",
    "    hires_steps=20\n",
    "):\n",
    "    \"\"\"Generate an image using noobai-XL-1.1 model with unlimited prompt length.\"\"\"\n",
    "    try:\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            random.seed(seed)\n",
    "            generator = torch.Generator(device=device).manual_seed(seed)\n",
    "        else:\n",
    "            generator = torch.Generator(device=device).manual_seed(random.randint(0, 2**32 - 1))\n",
    "        \n",
    "        pipe = load_pipeline_without_limits(sampler)\n",
    "        \n",
    "        with torch.autocast(device.type):\n",
    "            output = pipe(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                num_inference_steps=sampling_steps,\n",
    "                guidance_scale=cfg_scale,\n",
    "                height=image_size[1],\n",
    "                width=image_size[0],\n",
    "                generator=generator\n",
    "            )\n",
    "        \n",
    "        image = output.images[0]\n",
    "        \n",
    "        if hires_fix:\n",
    "            new_width = int(image_size[0] * hires_scale)\n",
    "            new_height = int(image_size[1] * hires_scale)\n",
    "            \n",
    "            with torch.autocast(device.type):\n",
    "                output = pipe(\n",
    "                    prompt=prompt,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                    num_inference_steps=hires_steps,\n",
    "                    guidance_scale=cfg_scale,\n",
    "                    height=new_height,\n",
    "                    width=new_width,\n",
    "                    generator=generator\n",
    "                )\n",
    "            \n",
    "            image = output.images[0]\n",
    "        \n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image: {str(e)}\")\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image generation with sample prompt\n",
    "prompt = \"\"\"\n",
    "masterpiece, best quality, ultra detailed, A majestic dragon soaring through storm clouds,\n",
    "lightning crackling around its wings, highly detailed scales showing intricate patterns and textures,\n",
    "epic lighting with multiple lightning bolts, volumetric storm clouds\n",
    "\"\"\"\n",
    "\n",
    "negative_prompt = \"\"\"\n",
    "worst quality, low quality, normal quality, lowres, low details, oversaturated,\n",
    "undersaturated, overexposed, underexposed, grainy, blur, blurry, text, watermark,\n",
    "signature, error, drawing, painting, illustration, anime, cartoon, stylized\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    'negative_prompt': negative_prompt,\n",
    "    'sampler': \"Euler\",\n",
    "    'sampling_steps': 30,\n",
    "    'seed': 42,\n",
    "    'cfg_scale': 7.0,\n",
    "    'image_size': (1024, 1024),\n",
    "    'hires_fix': True,\n",
    "    'hires_scale': 1.5,\n",
    "    'hires_steps': 20\n",
    "}\n",
    "\n",
    "try:\n",
    "    generated_image = generate_image(prompt, **params)\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(generated_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = f\"generated_image_{timestamp}_seed{params['seed']}.png\"\n",
    "    generated_image.save(output_path)\n",
    "    print(f\"Image saved to {output_path}\")\n",
    "    print(f\"Seed used: {params['seed']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to generate image: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
