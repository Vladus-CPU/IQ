{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Використання моделі noobai-XL-1.1 у Google Colab\n",
    "\n",
    "У цьому ноутбуці ми налаштуємо середовище для використання моделі `noobai-XL-1.1` з Hugging Face, завантажимо модель, та продемонструємо, як її використовувати для генерації тексту."
  ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Встановлення необхідних бібліотек\n",
    "!pip install transformers torch"
  ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Імпорт необхідних модулів\n",
    "\n",
    "Імпортуємо бібліотеки, необхідні для роботи з моделлю."
  ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
  ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Завантаження токенізатора та моделі\n",
    "\n",
    "Завантажимо токенізатор і модель з Hugging Face. Переконайтесь, що ви маєте доступ до моделі `noobai-XL-1.1`."
  ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Завантаження токенізатора\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Laxhar/noobai-XL-1.1\")\n",
    "\n",
    "# Завантаження моделі\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Laxhar/noobai-XL-1.1\")\n",
    "\n",
    "# Перевірка доступності GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
  ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Використання моделі для генерації тексту\n",
    "\n",
    "Нижче наведено приклад використання моделі для генерації тексту на основі заданого промпту. Ви можете налаштовувати параметри генерації відповідно до ваших потреб."
  ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Введення користувача\n",
    "prompt = \"Напишіть вступ до наукової статті про штучний інтелект:\"\n",
    "\n",
    "# Токенізація введення\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Генерація тексту\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_length=200,          # Максимальна довжина генерованого тексту\n",
    "    num_return_sequences=1,  # Кількість варіантів генерації\n",
    "    no_repeat_ngram_size=2,  # Уникнення повторення n-грам\n",
    "    temperature=0.7,         # Контроль креативності генерації\n",
    "    top_p=0.9,               # Ядерне вибірка для генерації\n",
    "    do_sample=True            # Використання семплінгу\n",
    ")\n",
    "\n",
    "# Декодування та вивід результату\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
  ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Налаштування параметрів генерації\n",
    "\n",
    "Ви можете експериментувати з різними параметрами генерації, щоб отримати бажані результати. Нижче наведено опис деяких ключових параметрів:\n",
    "\n",
    "- `max_length`: Визначає максимальну довжину генерованого тексту.\n",
    "- `num_return_sequences`: Кількість різних варіантів тексту, які генерує модель.\n",
    "- `temperature`: Контролює випадковість генерації. Менші значення роблять текст більш передбачуваним.\n",
    "- `top_p`: Використовується для ядерної вибірки, де `top_p=0.9` означає, що модель розглядає найімовірніші слова до тих пір, поки їх сума не досягне 90%.\n",
    "- `no_repeat_ngram_size`: Забезпечує, що n-грам певної довжини не повторюється в генерованому тексті.\n",
    "- `do_sample`: Визначає, чи використовувати семплінг (True) чи жорсткий пошук (False)."
  ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Додаткові ресурси\n",
    "\n",
    "- [Документація Transformers](https://huggingface.co/docs/transformers/index)\n",
    "- [Приклади використання моделей Hugging Face в Colab](https://huggingface.co/transformers/installation.html#google-colaboratory)"
  ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
