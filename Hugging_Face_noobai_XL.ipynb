{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlimited Token Stable Diffusion XL with noobai-XL-1.1\n",
    "\n",
    "**Author:** Vladus-CPU  \n",
    "**Last Updated:** 2025-01-01 22:08:31 UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --upgrade diffusers==0.21.4 transformers torch torchvision safetensors accelerate\n",
    "!pip install -U xformers --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline, EulerAncestralDiscreteScheduler, DDIMScheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check CUDA version if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pipeline_without_limits(scheduler_type=\"Euler\"):\n",
    "    \"\"\"Load the noobai-XL-1.1 pipeline with unlimited token length.\"\"\"\n",
    "    model_id = \"Laxhar/noobai-XL-1.1\"\n",
    "    \n",
    "    # Load pipeline with basic configuration\n",
    "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        use_safetensors=True,\n",
    "        variant=\"fp16\" if device.type == \"cuda\" else None\n",
    "    )\n",
    "    \n",
    "    # Remove token length limits\n",
    "    if hasattr(pipe, 'tokenizer') and pipe.tokenizer is not None:\n",
    "        pipe.tokenizer.model_max_length = int(1e9)\n",
    "        if hasattr(pipe.tokenizer, 'max_length'):\n",
    "            pipe.tokenizer.max_length = int(1e9)\n",
    "            \n",
    "    if hasattr(pipe, 'tokenizer_2') and pipe.tokenizer_2 is not None:\n",
    "        pipe.tokenizer_2.model_max_length = int(1e9)\n",
    "        if hasattr(pipe.tokenizer_2, 'max_length'):\n",
    "            pipe.tokenizer_2.max_length = int(1e9)\n",
    "    \n",
    "    # Set scheduler\n",
    "    if scheduler_type == \"Euler\":\n",
    "        pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "    elif scheduler_type == \"DDIM\":\n",
    "        pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown scheduler type\")\n",
    "    \n",
    "    # Try to enable memory optimizations\n",
    "    try:\n",
    "        import xformers\n",
    "        pipe.enable_xformers_memory_efficient_attention()\n",
    "        print(\"xformers optimization enabled\")\n",
    "    except ImportError:\n",
    "        print(\"xformers not available, using attention slicing instead\")\n",
    "        pipe.enable_attention_slicing()\n",
    "    \n",
    "    # Enable VAE tiling\n",
    "    if hasattr(pipe, \"enable_vae_tiling\"):\n",
    "        pipe.enable_vae_tiling()\n",
    "        print(\"VAE tiling enabled\")\n",
    "    \n",
    "    # Handle device placement\n",
    "    if device.type == \"cuda\":\n",
    "        if torch.cuda.get_device_properties(0).total_memory < 12 * 1024 * 1024 * 1024:\n",
    "            print(\"Low VRAM mode: enabling sequential CPU offload\")\n",
    "            pipe.enable_sequential_cpu_offload()\n",
    "        else:\n",
    "            pipe = pipe.to(device)\n",
    "            print(\"Model loaded to GPU\")\n",
    "    else:\n",
    "        pipe = pipe.to(device)\n",
    "        print(\"Model loaded to CPU\")\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(\n",
    "    prompt,\n",
    "    negative_prompt=\"\",\n",
    "    sampler=\"Euler\",\n",
    "    sampling_steps=30,\n",
    "    seed=None,\n",
    "    cfg_scale=7.0,\n",
    "    image_size=(1024, 1024),\n",
    "    hires_fix=False,\n",
    "    hires_scale=1.5,\n",
    "    hires_steps=20\n",
    "):\n",
    "    \"\"\"Generate an image using noobai-XL-1.1 model with unlimited prompt length.\"\"\"\n",
    "    try:\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            random.seed(seed)\n",
    "            generator = torch.Generator(device=device).manual_seed(seed)\n",
    "        else:\n",
    "            seed = random.randint(0, 2**32 - 1)\n",
    "            generator = torch.Generator(device=device).manual_seed(seed)\n",
    "        \n",
    "        print(f\"Using seed: {seed}\")\n",
    "        pipe = load_pipeline_without_limits(sampler)\n",
    "        \n",
    "        # Generate base image\n",
    "        with torch.autocast(device.type if device.type == \"cuda\" else \"cpu\"):\n",
    "            output = pipe(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                num_inference_steps=sampling_steps,\n",
    "                guidance_scale=cfg_scale,\n",
    "                height=image_size[1],\n",
    "                width=image_size[0],\n",
    "                generator=generator\n",
    "            )\n",
    "        \n",
    "        image = output.images[0]\n",
    "        \n",
    "        # Generate high-res version if requested\n",
    "        if hires_fix:\n",
    "            print(\"Generating high-resolution version...\")\n",
    "            new_width = int(image_size[0] * hires_scale)\n",
    "            new_height = int(image_size[1] * hires_scale)\n",
    "            \n",
    "            with torch.autocast(device.type if device.type == \"cuda\" else \"cpu\"):\n",
    "                output = pipe(\n",
    "                    prompt=prompt,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                    num_inference_steps=hires_steps,\n",
    "                    guidance_scale=cfg_scale,\n",
    "                    height=new_height,\n",
    "                    width=new_width,\n",
    "                    generator=generator\n",
    "                )\n",
    "            \n",
    "            image = output.images[0]\n",
    "        \n",
    "        # Clear CUDA cache if needed\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return image, seed\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image: {str(e)}\")\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image generation\n",
    "prompt = \"\"\"\n",
    "masterpiece, best quality, ultra detailed, A majestic dragon soaring through storm clouds,\n",
    "lightning crackling around its wings, highly detailed scales showing intricate patterns and textures,\n",
    "epic lighting with multiple lightning bolts, volumetric storm clouds\n",
    "\"\"\"\n",
    "\n",
    "negative_prompt = \"\"\"\n",
    "worst quality, low quality, normal quality, lowres, low details, oversaturated,\n",
    "undersaturated, overexposed, underexposed, grainy, blur, blurry, text, watermark,\n",
    "signature, error, drawing, painting, illustration, anime, cartoon, stylized\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    'negative_prompt': negative_prompt,\n",
    "    'sampler': \"Euler\",\n",
    "    'sampling_steps': 30,\n",
    "    'seed': None,  # Random seed\n",
    "    'cfg_scale': 7.0,\n",
    "    'image_size': (1024, 1024),\n",
    "    'hires_fix': True,\n",
    "    'hires_scale': 1.5,\n",
    "    'hires_steps': 20\n",
    "}\n",
    "\n",
    "try:\n",
    "    generated_image, used_seed = generate_image(prompt, **params)\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(generated_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = f\"generated_image_{timestamp}_seed{used_seed}.png\"\n",
    "    generated_image.save(output_path)\n",
    "    print(f\"Image saved to {output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to generate image: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
