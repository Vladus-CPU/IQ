{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NoobAI XL 1.1 - Text-to-Image Generation with Customizable Parameters\n",
    "\n",
    "This notebook allows you to use the [NoobAI XL 1.1](https://huggingface.co/Laxhar/noobai-XL-1.1) model for text-to-image generation on [Google Colab](https://colab.research.google.com/). You can customize various parameters such as sampler methods, sampling steps, seeds, CFG scale, image size, and apply high-resolution fixes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install torch==1.13.1+cu118 torchvision==0.14.1+cu118 torchaudio==0.13.1+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install diffusers transformers accelerate scipy ftfy\n",
    "!pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
    "from transformers import CLIPTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model with Customized Tokenizer Configuration\n",
    "\n",
    "We will modify the tokenizer configuration to remove the character limit by setting a high `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load and modify tokenizer configurations\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "# Function to load and modify tokenizer\n",
    "def load_modified_tokenizer(model_name, new_max_length=1000):\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    tokenizer.model_max_length = new_max_length\n",
    "    return tokenizer\n",
    "\n",
    "# Load both tokenizers with increased max_length\n",
    "tokenizer_1 = load_modified_tokenizer(\"Laxhar/noobai-XL-1.1\")\n",
    "tokenizer_2 = load_modified_tokenizer(\"Laxhar/noobai-XL-1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Pipeline with Custom Parameters\n",
    "\n",
    "Set up the Stable Diffusion pipeline with customizable parameters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define scheduler\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(\n",
    "    \"Laxhar/noobai-XL-1.1\",\n",
    "    subfolder=\"scheduler\",\n",
    ")\n",
    "\n",
    "# Load the pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"Laxhar/noobai-XL-1.1\",\n",
    "    scheduler=scheduler,\n",
    "    tokenizer=tokenizer_1,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    revision=\"main\"\n",
    ")\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Generation Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define default parameters\n",
    "sampler_methods = [\"Euler\", \"Heun\", \"DPM2\", \"DDIM\"]\n",
    "default_sampler = \"Euler\"\n",
    "default_steps = 50\n",
    "default_seed = 42\n",
    "default_cfg_scale = 7.5\n",
    "default_image_size = 512\n",
    "default_hires_fix = True\n",
    "hires_scale = 2.0\n",
    "hires_steps = 50\n",
    "hires_image_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Inputs for Customization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ipywidgets import interact, Dropdown, IntSlider, FloatSlider, Checkbox, Text\n",
    "import random\n",
    "\n",
    "def generate_image(\n",
    "    prompt,\n",
    "    sampler=default_sampler,\n",
    "    steps=default_steps,\n",
    "    seed=default_seed,\n",
    "    cfg_scale=default_cfg_scale,\n",
    "    image_size=default_image_size,\n",
    "    hires_fix=default_hires_fix,\n",
    "    hires_scale=hires_scale,\n",
    "    hires_steps=hires_steps,\n",
    "    hires_image_size=hires_image_size\n",
    "):\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 1000000)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    \n",
    "    # Update scheduler based on sampler\n",
    "    if sampler == \"Euler\":\n",
    "        pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "    \n",
    "    # Generate image\n",
    "    image = pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=steps,\n",
    "        guidance_scale=cfg_scale,\n",
    "        generator=generator,\n",
    "        height=image_size,\n",
    "        width=image_size\n",
    "    ).images[0]\n",
    "    \n",
    "    # High-resolution fix\n",
    "    if hires_fix:\n",
    "        image = image.resize((hires_image_size, hires_image_size), Image.LANCZOS)\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "interact(\n",
    "    generate_image,\n",
    "    prompt=Text(value='A futuristic cityscape at sunset', description='Prompt:'),\n",
    "    sampler=Dropdown(options=sampler_methods, value=default_sampler, description='Sampler:'),\n",
    "    steps=IntSlider(value=default_steps, min=10, max=100, step=10, description='Steps:'),\n",
    "    seed=IntSlider(value=default_seed, min=-1, max=1000000, step=1, description='Seed (-1 for random):'),\n",
    "    cfg_scale=FloatSlider(value=default_cfg_scale, min=1.0, max=20.0, step=0.5, description='CFG Scale:'),\n",
    "    image_size=IntSlider(value=default_image_size, min=256, max=1024, step=64, description='Image Size:'),\n",
    "    hires_fix=Checkbox(value=default_hires_fix, description='High-Res Fix'),\n",
    "    hires_scale=FloatSlider(value=hires_scale, min=1.0, max=4.0, step=0.1, description='HiRes Scale:'),\n",
    "    hires_steps=IntSlider(value=hires_steps, min=10, max=100, step=10, description='HiRes Steps:'),\n",
    "    hires_image_size=IntSlider(value=hires_image_size, min=512, max=2048, step=128, description='HiRes Size:')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
