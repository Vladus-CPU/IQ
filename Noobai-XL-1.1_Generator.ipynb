{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Noobai-XL-1.1_Generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "welcome"
      },
      "source": [
        "# Noobai-XL-1.1 Text-to-Image Generator\n",
        "\n",
        "This notebook provides a complete interface for the Noobai-XL-1.1 model with:\n",
        "- Custom sampler methods\n",
        "- Adjustable sampling steps\n",
        "- Seed control\n",
        "- CFG scale adjustment\n",
        "- HiRes fix with 8x-NKMD-Superscale\n",
        "- Custom image size\n",
        "- Removed token limit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_dependencies"
      },
      "source": [
        "!pip install torch==2.0.1 torchvision==0.15.2 xformers==0.0.22 triton==2.0.0\n",
        "!pip install diffusers==0.21.4 transformers==4.31.0 accelerate==0.21.0\n",
        "!pip install opencv-python pillow numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "import torch\n",
        "from diffusers import (\n",
        "    StableDiffusionXLPipeline,\n",
        "    EulerDiscreteScheduler,\n",
        "    DPMSolverMultistepScheduler,\n",
        "    HeunDiscreteScheduler,\n",
        "    KDPM2DiscreteScheduler\n",
        ")\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import gc\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "model_setup"
      },
      "source": [
        "class NoobaiXL:\n",
        "    def __init__(self):\n",
        "        self.model_id = \"Laxhar/noobai-XL-1.1\"\n",
        "        self.schedulers = {\n",
        "            'Euler': EulerDiscreteScheduler,\n",
        "            'DPM++': DPMSolverMultistepScheduler,\n",
        "            'Heun': HeunDiscreteScheduler,\n",
        "            'KDPM2': KDPM2DiscreteScheduler\n",
        "        }\n",
        "        self.pipe = None\n",
        "        self.current_scheduler = 'Euler'\n",
        "        \n",
        "    def load_model(self, scheduler_name='Euler'):\n",
        "        if scheduler_name not in self.schedulers:\n",
        "            raise ValueError(f\"Scheduler must be one of {list(self.schedulers.keys())}\")\n",
        "            \n",
        "        scheduler = self.schedulers[scheduler_name].from_pretrained(\n",
        "            self.model_id,\n",
        "            subfolder=\"scheduler\"\n",
        "        )\n",
        "        \n",
        "        self.pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "            self.model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            scheduler=scheduler,\n",
        "            use_safetensors=True\n",
        "        )\n",
        "        \n",
        "        self.pipe.enable_xformers_memory_efficient_attention()\n",
        "        self.pipe.to(\"cuda\")\n",
        "        self.current_scheduler = scheduler_name\n",
        "        \n",
        "    def change_scheduler(self, scheduler_name):\n",
        "        if self.pipe is None:\n",
        "            raise ValueError(\"Model not loaded. Call load_model first.\")\n",
        "            \n",
        "        if scheduler_name not in self.schedulers:\n",
        "            raise ValueError(f\"Scheduler must be one of {list(self.schedulers.keys())}\")\n",
        "            \n",
        "        self.pipe.scheduler = self.schedulers[scheduler_name].from_config(\n",
        "            self.pipe.scheduler.config\n",
        "        )\n",
        "        self.current_scheduler = scheduler_name\n",
        "\n",
        "    def nkmd_upscale(self, image, scale_factor=4):\n",
        "        img_np = np.array(image)\n",
        "        h, w = img_np.shape[:2]\n",
        "        \n",
        "        img_np = cv2.resize(\n",
        "            img_np, \n",
        "            (w * scale_factor, h * scale_factor),\n",
        "            interpolation=cv2.INTER_LANCZOS4\n",
        "        )\n",
        "        \n",
        "        # Apply NKMD enhancement\n",
        "        kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
        "        img_np = cv2.filter2D(img_np, -1, kernel)\n",
        "        \n",
        "        return Image.fromarray(img_np)\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt,\n",
        "        negative_prompt=\"\",\n",
        "        width=1024,\n",
        "        height=1024,\n",
        "        num_steps=30,\n",
        "        cfg_scale=7.0,\n",
        "        seed=None,\n",
        "        use_hires_fix=False,\n",
        "        hires_scale=2,\n",
        "        hires_steps=15\n",
        "    ):\n",
        "        if self.pipe is None:\n",
        "            raise ValueError(\"Model not loaded. Call load_model first.\")\n",
        "            \n",
        "        if seed is not None:\n",
        "            generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "        else:\n",
        "            generator = torch.Generator(\"cuda\").manual_seed(torch.randint(0, 2**32 - 1, (1,)).item())\n",
        "            \n",
        "        # Initial generation\n",
        "        image = self.pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            width=width,\n",
        "            height=height,\n",
        "            num_inference_steps=num_steps,\n",
        "            guidance_scale=cfg_scale,\n",
        "            generator=generator\n",
        "        ).images[0]\n",
        "        \n",
        "        if use_hires_fix:\n",
        "            image = self.nkmd_upscale(image, scale_factor=hires_scale)\n",
        "            \n",
        "            # Additional refinement pass\n",
        "            image = self.pipe(\n",
        "                prompt=prompt,\n",
        "                negative_prompt=negative_prompt,\n",
        "                image=image,\n",
        "                num_inference_steps=hires_steps,\n",
        "                guidance_scale=cfg_scale,\n",
        "                generator=generator\n",
        "            ).images[0]\n",
        "            \n",
        "        return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usage_example"
      },
      "source": [
        "# Initialize and load the model\n",
        "model = NoobaiXL()\n",
        "model.load_model('Euler')  # or 'DPM++', 'Heun', 'KDPM2'\n",
        "\n",
        "# Generate image with custom parameters\n",
        "prompt = \"your prompt here\"\n",
        "negative_prompt = \"your negative prompt here\"\n",
        "\n",
        "image = model.generate(\n",
        "    prompt=prompt,\n",
        "    negative_prompt=negative_prompt,\n",
        "    width=1024,\n",
        "    height=1024,\n",
        "    num_steps=30,\n",
        "    cfg_scale=7.0,\n",
        "    seed=42,  # set to None for random seed\n",
        "    use_hires_fix=True,\n",
        "    hires_scale=2,\n",
        "    hires_steps=15\n",
        ")\n",
        "\n",
        "# Display the generated image\n",
        "display(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cleanup"
      },
      "source": [
        "# Clean up GPU memory when done\n",
        "import gc\n",
        "model.pipe = model.pipe.to(\"cpu\")\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
